# Workflow preprocesses WGS sequences in FASTQ or BAM, then
# runs original PCAWG BWA-MEM alignment, different QC metrics are 
# also collected at convenient steps

workflow:
  name:  bwa-mem-wgs-aligner
  version: "0.0.25"

  env_var:  # JT will not pass these as parameters for tasks, instead, JT may check existence of the env variable
    SCORE_TOKEN:
      type: string
      is_required: true
    SONG_TOKEN:
      type: string
      is_required: true
  input:
    song_collab_url:
      type: string
      default: "https://song.cancercollaboratory.org"
    song_aws_url:
      type: string
      default: "https://virginia.song.icgc.org"
    storage_collab_url:
      type: string
      default: ""
    storage_aws_url:
      type: string
      default: ""
    metadata_yaml:
      type: string
      is_file: true
      is_required: true
    picard_jar:
      type: string
      is_file: true
      default: "[picard.jar]https://github.com/broadinstitute/picard/releases/download/2.18.16/picard.jar"
    dckr_pcawg2_song:
      type: string
      default: quay.io/pancancer/song-uperations:1.0.0
    dckr_score:
      type: string
      default: overture/score:1.5.0  # need to verify
    pcawg_aligner_dockstore_url:
      type: string
      is_file: true
      default: "[pcawg-bwa-mem.cwl]https://dockstore.org/api/api/ga4gh/v2/tools/quay.io%2Fpancancer%2Fpcawg-bwa-mem-workflow/versions/2.6.8_1.4/plain-CWL/descriptor/Dockstore.cwl"
    reference_gz_amb:
      type: string
      is_file: true
      default: "[genome.fa.gz.64.amb]https://dcc.icgc.org/api/v1/download?fn=/PCAWG/reference_data/pcawg-bwa-mem/genome.fa.gz.64.amb"
    reference_gz_sa:
      type: string
      is_file: true
      default: "[genome.fa.gz.64.sa]https://dcc.icgc.org/api/v1/download?fn=/PCAWG/reference_data/pcawg-bwa-mem/genome.fa.gz.64.sa"
    reference_gz_pac:
      type: string
      is_file: true
      default: "[genome.fa.gz.64.pac]https://dcc.icgc.org/api/v1/download?fn=/PCAWG/reference_data/pcawg-bwa-mem/genome.fa.gz.64.pac"
    reference_gz_ann:
      type: string
      is_file: true
      default: "[genome.fa.gz.64.ann]https://dcc.icgc.org/api/v1/download?fn=/PCAWG/reference_data/pcawg-bwa-mem/genome.fa.gz.64.ann"
    reference_gz_bwt:
      type: string
      is_file: true
      default: "[genome.fa.gz.64.bwt]https://dcc.icgc.org/api/v1/download?fn=/PCAWG/reference_data/pcawg-bwa-mem/genome.fa.gz.64.bwt"
    reference_gz_fai:
      type: string
      is_file: true
      default: "[genome.fa.gz.fai]https://dcc.icgc.org/api/v1/download?fn=/PCAWG/reference_data/pcawg-bwa-mem/genome.fa.gz.fai"
    reference_gz:
      type: string
      is_file: true
      default: "[genome.fa.gz]https://dcc.icgc.org/api/v1/download?fn=/PCAWG/reference_data/pcawg-bwa-mem/genome.fa.gz"

  tasks:
    validate_metadata:
      tool: validate_metadata
      input:
        metadata_yaml: metadata_yaml

    download:
      tool: download
      input:
        metadata_json: metadata_json@validate_metadata
        input_format: input_format@validate_metadata

    fastq_to_sam:
      tool: fastq_to_sam
      input:
        metadata_json: metadata_json@validate_metadata
        download_files: download_files@download
        picard_jar: picard_jar
        input_format: input_format@validate_metadata

    revert_bam:
      tool: revert_bam
      input:
        metadata_json: metadata_json@validate_metadata
        download_files: download_files@download
        picard_jar: picard_jar
        input_format: input_format@validate_metadata

    replace_readgroup:
      tool: replace_readgroup
      input:
        metadata_json: metadata_json@validate_metadata
        picard_jar: picard_jar
        input_format: input_format@validate_metadata
        unaligned_by_rg_dir: unaligned_by_rg_dir@revert_bam

    add_comment:
      tool: add_comment
      input:
        metadata_json: metadata_json@validate_metadata
        bams: bams@fastq_to_sam
        picard_jar: picard_jar
        input_format: input_format@validate_metadata
        unaligned_rg_replace_dir: unaligned_rg_replace_dir@replace_readgroup

    lane_bam_qc:
      tool: lane_bam_qc
      input:
        lane_bams: bams@add_comment
        picard_jar: picard_jar

    pcawg_aligner:
      tool: pcawg_aligner
      input:
        cwl_file: pcawg_aligner_dockstore_url
        lane_bams: bams@add_comment
        output_file_basename: aligned_bam_basename@add_comment
        reference_gz_amb: reference_gz_amb
        reference_gz_sa: reference_gz_sa
        reference_gz_pac: reference_gz_pac
        reference_gz_ann: reference_gz_ann
        reference_gz_bwt: reference_gz_bwt
        reference_gz_fai: reference_gz_fai
        reference_gz: reference_gz

    aligned_bam_qc:
      tool: aligned_bam_qc
      input:
        picard_jar: picard_jar
        aligned_bam: merged_output_bam@pcawg_aligner
        reference_sequence: reference_gz  # may need unzipped version

    aligned_bam_oxog_metrics:
      tool: aligned_bam_oxog_metrics
      input:
        picard_jar: picard_jar
        aligned_bam: merged_output_bam@pcawg_aligner
        reference_sequence: reference_gz  # may need unzipped version

    # may add more QCs if needed

    create_tar:
      tool: create_tar
      input:
        aliquot_id: aliquot_id@validate_metadata
        number_of_lanes: number_of_lanes@validate_metadata
        dckr_pcawg2_song: dckr_pcawg2_song
        output_dir: output_dir@pcawg_aligner
        task_aligned_bam_qc_outdir: output_dir@aligned_bam_qc
        lane_unaligned_dir: unaligned_by_rg_dir@revert_bam
        task_aligned_bam_oxog_metrics_wkdir: output_dir@aligned_bam_oxog_metrics


    generate_song_payload:
      tool: generate_song_payload
      input:
        dckr_pcawg2_song: dckr_pcawg2_song
        metadata_yaml: metadata_yaml
        bam_file: merged_output_bam@pcawg_aligner
        bai_file: merged_output_bai@pcawg_aligner
        tar_file: tar_file@create_tar
        task_aligned_bam_qc_outdir: output_dir@aligned_bam_qc
        lane_unaligned_dir: unaligned_by_rg_dir@revert_bam
        task_aligned_bam_oxog_metrics_wkdir: output_dir@aligned_bam_oxog_metrics


    upload_song_payload_collab:
      tool: upload_song_payload
      input:
        is_allowed: collab_upload_allowed@validate_metadata
        payload: payload@generate_song_payload
        song_metadata_url: song_collab_url
        study: study@validate_metadata

    upload_song_payload_aws:
      tool: upload_song_payload
      input:
        is_allowed: aws_upload_allowed@validate_metadata
        payload: payload@generate_song_payload
        song_metadata_url: song_aws_url
        study: study@validate_metadata

    save_song_payload_collab:
      tool: save_song_payload
      input:
        is_allowed: collab_upload_allowed@validate_metadata
        song_metadata_url: song_collab_url
        study: study@validate_metadata
        upload_id: uploadId@upload_song_payload_collab

    save_song_payload_aws:
      tool: save_song_payload
      input:
        is_allowed: aws_upload_allowed@validate_metadata
        song_metadata_url: song_aws_url
        study: study@validate_metadata
        upload_id: uploadId@upload_song_payload_aws

    create_manifest_file_collab:
      tool: create_manifest_file
      input:
        is_allowed: collab_upload_allowed@validate_metadata
        input_dir: output_dir@pcawg_aligner
        song_metadata_url: song_collab_url
        study: study@validate_metadata
        analysis_id: analysis_id@save_song_payload_collab

    create_manifest_file_aws:
      tool: create_manifest_file
      input:
        is_allowed: aws_upload_allowed@validate_metadata
        input_dir: output_dir@pcawg_aligner
        song_metadata_url: song_aws_url
        study: study@validate_metadata
        analysis_id: analysis_id@save_song_payload_aws

    score_upload_collab:
      tool: score_upload
      input:
        is_allowed: collab_upload_allowed@validate_metadata
        manifest_file: manifest_file@create_manifest_file_collab
        input_dir: output_dir@pcawg_aligner
        song_metadata_url: song_collab_url
        storage_url: storage_collab_url

    score_upload_aws:
      tool: score_upload
      input:
        is_allowed: aws_upload_allowed@validate_metadata
        manifest_file: manifest_file@create_manifest_file_aws
        input_dir: output_dir@pcawg_aligner
        song_metadata_url: song_aws_url
        storage_url: storage_aws_url

    publish_song_payload_collab:
      tool: publish_song_payload
      input:
        is_allowed: collab_upload_allowed@validate_metadata
        song_metadata_url: song_collab_url
        study: study@validate_metadata
        analysis_id: analysis_id@save_song_payload_collab

    publish_song_payload_aws:
      tool: publish_song_payload
      input:
        is_allowed: aws_upload_allowed@validate_metadata
        song_metadata_url: song_aws_url
        study: study@validate_metadata
        analysis_id: analysis_id@save_song_payload_aws

# A workflow is made up with one or more tools
# Each tool can have its own docker imagine if desirable
tools:
  validate_metadata:
    command: validate_metadata.py

    input:
      metadata_yaml:
        type: string
        is_file: true

    output:
      input_format:
        type: string
      metadata_json:
        type: string
        is_file: true
      aliquot_id:
        type: string
      number_of_lanes:
        type: integer
      study:
        type: string
      collab_upload_allowed:
        type: string
      aws_upload_allowed:
        type: string
      cgc_upload_allowed:
        type: string

  download:
    command: download.py

    input:
      metadata_json:
        type: string
        is_file: true
      input_format:
        type: string

    output:
      output_dir:
        type: string
      download_files:
        type: array
        items:
          type: object

  fastq_to_sam:
    command: fastq_to_sam.py

    input:
      metadata_json:
        type: string
        is_file: true
      picard_jar:
        type: string
        is_file: true
      download_files:
        type: array
        items:
          type: object
      input_format:
        type: string

    output:
      bams:
        type: array
        items:
          type: string
          is_file: true
          glob_pattern: "*.bam"

  revert_bam:
    command: revert_bam.py

    input:
      metadata_json:
        type: string
        is_file: true
      picard_jar:
        type: string
        is_file: true
      download_files:
        type: array
        items:
          type: object
      input_format:
        type: string

    output:
      unaligned_by_rg_dir:
        type: string

  replace_readgroup:
    command: replace_readgroup.py

    input:
      metadata_json:
        type: string
        is_file: true
      picard_jar:
        type: string
        is_file: true
      unaligned_by_rg_dir:
        type: string
      input_format:
        type: string

    output:
      unaligned_rg_replace_dir:
        type: string

  add_comment:
    command: add_comment.py

    input:
      metadata_json:
        type: string
        is_file: true
      picard_jar:
        type: string
        is_file: true
      unaligned_rg_replace_dir:
        type: string
      input_format:
        type: string
      bams:
        type: array
        items:
          type: string
          is_file: true
          glob_pattern: "*.bam"

    output:
      aligned_bam_basename:
        type: string
      bams:
        type: array
        items:
          type: string
          is_file: true
          glob_pattern: "*.bam"

  lane_bam_qc:
    command: |
      python -c '
      import sys
      import json
      import subprocess
      lane_bams = sys.argv[1].split(",")

      metrics = []
      for bam in lane_bams:
        metrics_file = "%s.quality_yield_metrics.txt" % bam
        command = "java -jar ${picard_jar} CollectQualityYieldMetrics I=%s O=%s" % (bam, metrics_file)
        try:
          p = subprocess.Popen([command], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
          stdout, stderr = p.communicate()
        except Exception as e:
          exit(stderr)
        metrics.append(metrics_file)

      print(json.dumps({
        "metrics": metrics
      }))

      ' ${sep=',' lane_bams} > output.json 2> std.err

    input:
      picard_jar:
        type: string
        is_file: true
      lane_bams:
        type: array
        items:
          type: string
          is_file: true
    output:
      metrics:
        type: array
        items:
          type: string
          is_file: true
          glob_pattern: "*.quality_yield_metrics.txt"

  pcawg_aligner:  # this is really a PCAWG BWA-MEM CWL workflow wrapper
    command: |
      pcawg_aligner.py --cwl_file ${cwl_file} \
        --reads ${sep=' ' lane_bams} \
        --output_dir datastore \
        --output_file_basename ${output_file_basename} \
        --reference_gz_amb ${reference_gz_amb} \
        --reference_gz_sa ${reference_gz_sa} \
        --reference_gz_pac ${reference_gz_pac} \
        --reference_gz_ann ${reference_gz_ann} \
        --reference_gz_bwt ${reference_gz_bwt} \
        --reference_gz_fai ${reference_gz_fai} \
        --reference_gz ${reference_gz}
    input:
      cwl_file:
        type: string
        is_file: true
      lane_bams:
        type: array
        items:
          type: string
          is_file: true
      output_file_basename:
        type: string
      reference_gz_amb:
        type: string
        is_file: true
      reference_gz_sa:
        type: string
        is_file: true
      reference_gz_pac:
        type: string
        is_file: true
      reference_gz_ann:
        type: string
        is_file: true
      reference_gz_bwt:
        type: string
        is_file: true
      reference_gz_fai:
        type: string
        is_file: true
      reference_gz:
        type: string
        is_file: true
    output:  # output section is ignored for now
      output_dir:
        type: string
      merged_output_bam:
        type: string
        is_file: true
        value: "${output_file_basename}.bam"
      merged_output_bai:
        type: string
        is_file: true
        value: "${output_file_basename}.bam.bai"
      merged_output_metrics:
        type: string
        is_file: true
        value: "${output_file_basename}.bam.metrics"
      merged_output_stats:
        type: string
        is_file: true
        value: "${output_file_basename}.bam.stats.txt"

  aligned_bam_qc:  # this requires RScript, install it by 'apt install r-base-core'
    command: |
      java -Xms5000m -jar ${picard_jar} CollectMultipleMetrics \
        I=${aligned_bam} \
        O=multiple_metrics \
        R=${reference_sequence} \
        ASSUME_SORTED=true \
        PROGRAM="null" \
        PROGRAM=CollectBaseDistributionByCycle \
        PROGRAM=CollectAlignmentSummaryMetrics \
        PROGRAM=CollectInsertSizeMetrics \
        PROGRAM=MeanQualityByCycle \
        PROGRAM=QualityScoreDistribution \
        PROGRAM=CollectSequencingArtifactMetrics \
        PROGRAM=CollectQualityYieldMetrics \
        METRIC_ACCUMULATION_LEVEL="null" \
        METRIC_ACCUMULATION_LEVEL="ALL_READS" \
        METRIC_ACCUMULATION_LEVEL="SAMPLE" \
        METRIC_ACCUMULATION_LEVEL="LIBRARY" \
        METRIC_ACCUMULATION_LEVEL="READ_GROUP" \
      && echo "{ \"output_dir\": \"$(pwd)\" }" > output.json
    input:
      picard_jar:
        type: string
        is_file: true
      aligned_bam:
        type: string
      reference_sequence:
        type: string
        is_file: true
    output:
      output_dir:
        type: string
      metrics_files:
        type: array
        items:
          type: string
          is_file: true
          glob_pattern: "multiple_metrics*.txt"

  aligned_bam_oxog_metrics:
    command: |
      java -jar ${picard_jar} CollectOxoGMetrics \
        I=${aligned_bam} \
        O=oxoG_metrics.txt \
        R=${reference_sequence} \
      && echo "{ \"output_dir\": \"$(pwd)\" }" > output.json
    input:
      picard_jar:
        type: string
        is_file: true
      aligned_bam:
        type: string
      reference_sequence:
        type: string
        is_file: true
    output:
      output_dir:
        type: string

  create_tar:
    command: |
      TAR_NAME=${aliquot_id}.${number_of_lanes}.$(date +%Y%m%d).wgs.qc_metrics.tgz \
      && docker_pull ${dckr_pcawg2_song} \
      && docker run \
          --user 1000:1000 \
          --workdir / \
          -v ${output_dir}:/data \
          -v ${task_aligned_bam_qc_outdir}:/aligned_bam_qc \
          -v ${lane_unaligned_dir}:/unaligned_seq_qc \
          -v ${task_aligned_bam_oxog_metrics_wkdir}:/oxog_metrics \
          ${dckr_pcawg2_song} tar czf \
            /data/$TAR_NAME \
            aligned_bam_qc/multiple_metrics.* \
            unaligned_seq_qc/*.lane.bam.quality_yield_metrics.txt \
            oxog_metrics/oxoG_metrics.txt \
      && echo "{ \"tar_file\": \"${output_dir}/$TAR_NAME\" }" > output.json

    input:
      aliquot_id:
        type: string
      number_of_lanes:
        type: integer
      dckr_pcawg2_song:
        type: string
      output_dir:
        type: string
      task_aligned_bam_qc_outdir:
        type: string
      lane_unaligned_dir:
        type: string
      task_aligned_bam_oxog_metrics_wkdir:
        type: string

    output:
      tar_file:
        type: string
        is_file: true

  generate_song_payload:
    command: |
      docker_pull ${dckr_pcawg2_song} \
      && docker run \
        -v $(pwd):/data \
        -v ${metadata_yaml}:/metadata.yaml:ro \  # should these file be mounted directly under root?
        -v ${bam_file}:/$(basename ${bam_file}) \
        -v ${bai_file}:/$(basename ${bai_file}) \
        -v ${tar_file}:/$(basename {tar_file}) \
        -v ${lane_unaligned_dir}:/lane_unaligned \
        -v ${task_aligned_bam_oxog_metrics_wkdir}:/task_aligned_bam_oxog_metrics_wkdir \
        -v ${task_aligned_bam_qc_wkdir}:/task_aligned_bam_qc_wkdir \
        ${dckr_pcawg2_song} python3 generate_song_payload.py \
          /metadata.yaml \
          /$bam_file \
          /$bai_file \
          /$tar_file \
          /lane_unaligned \
          /task_aligned_bam_oxog_metrics_wkdir \
          /task_aligned_bam_qc_outdir | jq > payload.json \
      && echo "{ \"payload\": \"$(pwd)/payload.json\" }" > output.json

    input:
      dckr_pcawg2_song:
        type: string
      metadata_yaml:
        type: string
        is_file: true
      bam_file:
        type: string
        is_file: true
      bai_file:
        type: string
        is_file: true
      tar_file:
        type: string
        is_file: true
      lane_unaligned_dir:
        type: string
      task_aligned_bam_oxog_metrics_wkdir:
        type: string
      task_aligned_bam_qc_outdir:
        type: string
    output:
      payload:
        type: string
        is_file: true

  upload_song_payload:
    command: |
      if [ "${is_allowed}" = true ]; then \
        docker pull ${dckr_pcawg2_song} \
        && docker run \
            --net=host \
            -e ACCESS_TOKEN \
            -v ${payload}:/data/payload.json \
            ${dckr_pcawg2_song} \
            uperation sing upload \
              --song-url ${song_metadata_url} \
              --study ${study} \
              --payload /data/payload.json > output.json; \
      else \
        echo "{ \"_skipped\": true }" > output.json; \
      fi

    input:
      is_allowed:
        type: tring
      payload:
        type: string
        is_file: true
      song_metadata_url:
        type: string
      study:
        type: string
      dckr_pcawg2_song:
        type: string
    output:
      status:
        type: string
      uploadId:
        type: string

  save_song_payload:
    command: |
      if [ "${is_allowed}" = true ]; then \
        docker pull ${dckr_pcawg2_song} \
        && docker run \
            --net=host \
            -e ACCESS_TOKEN \
            ${dckr_pcawg2_song} \
            uperation sing save \
              --song-url ${song_metadata_url} \
              --study ${study} \
              --upload-id ${upload_id} > output.json; \
      else \
        echo "{ \"_skipped\": true }" > output.json; \
      fi

    input:
      is_allowed:
        type: tring
      dckr_pcawg2_song:
        type: string
      song_metadata_url:
        type: string
      study:
        type: string
      upload_id:
        type: string
    output:
      status:
        type: string
      analysis_id:
        type: string

  create_manifest_file:
    command: |
      if [ "${is_allowed}" = true ]; then \
        docker pull ${dckr_pcawg2_song} \
        && docker run \
            --net=host \
            -v $(pwd):/output \
            -v ${input_dir}:/data \
            ${dckr_pcawg2_song} \
            uperation sing manifest \
              --song-url ${song_metadata_url} \
              --study ${study} \
              --analysis ${analysis_id} \
              --files-dir ./ \
              --manifest $(echo -n 'manifest.txt')} \
        && echo "{ \"manifest_file\": \"$(pwd)/manifest.txt\" }" > output.json; \
      else \
        echo "{ \"_skipped\": true }" > output.json; \
      fi

    input:
      is_allowed:
        type: tring
      dckr_pcawg2_song:  # Docker image
        type: string
      input_dir:
        type: string
      song_metadata_url:
        type: string
      study:
        type: string
      analysis_id:
        type: string
    output:
      manifest_file:
        type: string
        is_file: true

  score_upload:
    command: |
      if [ "${is_allowed}" = true ]; then \
        docker pull ${dckr_score} \
        && docker run \
            --net=host \
            -v ${manifest_file}:/manifest.txt \
            -v ${input_dir}:/data \
            -e ACCESS_TOKEN \
            -e STORAGE_URL=${storage_url} \
            -e METADATA_URL=${song_metadata_url} \
            ${dckr_score} \
            bin/score-client upload \
              --manifest /manifest.txt --force; \
      else \
        echo "{ \"_skipped\": true }" > output.json; \
      fi

    input:
      is_allowed:
        type: tring
      dckr_score:
        type: string
      manifest_file:
        type: string
        is_file: true
      input_dir:
        type: string
      song_metadata_url:
        type: string
      storage_url:
        type: string

  publish_song_payload:
    command: |
      if [ "${is_allowed}" = true ]; then \
        docker pull ${dckr_pcawg2_song} \
        && docker run \
            --net=host \
            -e ACCESS_TOKEN \
            ${dckr_pcawg2_song} \
            uperation publish \
              --song-url ${song_metadata_url} \
              --study ${study} \
              --analysis-id ${analysis_id}; \
      else \
        echo "{ \"_skipped\": true }" > output.json; \
      fi

    input:
      is_allowed:
        type: tring
      dckr_pcawg2_song:  # Docker image
        type: string
      song_metadata_url:
        type: string
      study:
        type: string
      analysis_id:
        type: string
