# Workflow preprocesses WGS sequences in FASTQ or BAM, then
# runs original PCAWG BWA-MEM alignment, different QC metrics are 
# also collected at convenient steps

workflow:
  name:  bwa-mem-wgs-aligner
  version: "0.0.44"

  env_var:  # JT will not pass these as parameters for tasks, instead, JT may check existence of the env variable
    SCORE_TOKEN:
      type: string
      is_required: true
    SONG_TOKEN:
      type: string
      is_required: true
  input:
    song_collab_url:
      type: string
      default: "https://song.cancercollaboratory.org"
    song_aws_url:
      type: string
      default: "https://virginia.song.icgc.org"
    metadata_yaml:
      type: string
      is_file: true
      is_required: true
    picard_jar:
      type: string
      is_file: true
      default: "[picard.jar]https://github.com/broadinstitute/picard/releases/download/2.18.16/picard.jar"
    util_dckr:
      type: string
      default: quay.io/pancancer/util_dckr:0.0.5
    pcawg_aligner_dockstore_url:
      type: string
      is_file: true
      default: "[pcawg-bwa-mem.cwl]https://raw.githubusercontent.com/ICGC-TCGA-PanCancer/Seqware-BWA-Workflow/2.6.8_1.4/Dockstore.cwl"  # commit hash: 8d259bc2d616a7af5266e870ef8da071337eee21
      # default: "[pcawg-bwa-mem.cwl]https://dockstore.org/api/api/ga4gh/v2/tools/quay.io%2Fpancancer%2Fpcawg-bwa-mem-workflow/versions/2.6.8_1.4/plain-CWL/descriptor/Dockstore.cwl"
    reference_gz_amb:
      type: string
      is_file: true
      default: "[genome.fa.gz.64.amb]https://dcc.icgc.org/api/v1/download?fn=/PCAWG/reference_data/pcawg-bwa-mem/genome.fa.gz.64.amb"
    reference_gz_sa:
      type: string
      is_file: true
      default: "[genome.fa.gz.64.sa]https://dcc.icgc.org/api/v1/download?fn=/PCAWG/reference_data/pcawg-bwa-mem/genome.fa.gz.64.sa"
    reference_gz_pac:
      type: string
      is_file: true
      default: "[genome.fa.gz.64.pac]https://dcc.icgc.org/api/v1/download?fn=/PCAWG/reference_data/pcawg-bwa-mem/genome.fa.gz.64.pac"
    reference_gz_ann:
      type: string
      is_file: true
      default: "[genome.fa.gz.64.ann]https://dcc.icgc.org/api/v1/download?fn=/PCAWG/reference_data/pcawg-bwa-mem/genome.fa.gz.64.ann"
    reference_gz_bwt:
      type: string
      is_file: true
      default: "[genome.fa.gz.64.bwt]https://dcc.icgc.org/api/v1/download?fn=/PCAWG/reference_data/pcawg-bwa-mem/genome.fa.gz.64.bwt"
    reference_gz_fai:
      type: string
      is_file: true
      default: "[genome.fa.gz.fai]https://dcc.icgc.org/api/v1/download?fn=/PCAWG/reference_data/pcawg-bwa-mem/genome.fa.gz.fai"
    reference_gz:
      type: string
      is_file: true
      default: "[genome.fa.gz]https://dcc.icgc.org/api/v1/download?fn=/PCAWG/reference_data/pcawg-bwa-mem/genome.fa.gz"
    cgc_project_name:
      type: string

  tasks:
    validate_metadata:
      tool: validate_metadata
      input:
        metadata_yaml: metadata_yaml

    download:
      tool: download
      input:
        metadata_json: metadata_json@validate_metadata
        input_format: input_format@validate_metadata

    fastq_to_sam:
      tool: fastq_to_sam
      input:
        metadata_json: metadata_json@validate_metadata
        download_files: download_files@download
        picard_jar: picard_jar
        input_format: input_format@validate_metadata

    revert_bam:
      tool: revert_bam
      input:
        metadata_json: metadata_json@validate_metadata
        download_files: download_files@download
        picard_jar: picard_jar
        input_format: input_format@validate_metadata

    replace_readgroup:
      tool: replace_readgroup
      input:
        metadata_json: metadata_json@validate_metadata
        picard_jar: picard_jar
        input_format: input_format@validate_metadata
        unaligned_by_rg_dir: unaligned_by_rg_dir@revert_bam

    add_comment:
      tool: add_comment
      input:
        metadata_json: metadata_json@validate_metadata
        bams: bams@fastq_to_sam
        picard_jar: picard_jar
        input_format: input_format@validate_metadata
        unaligned_rg_replace_dir: unaligned_rg_replace_dir@replace_readgroup

    lane_bam_qc:
      tool: lane_bam_qc
      input:
        lane_bams: bams@add_comment
        picard_jar: picard_jar

    pcawg_aligner:
      tool: pcawg_aligner
      input:
        cwl_file: pcawg_aligner_dockstore_url
        lane_bams: bams@add_comment
        output_file_basename: aligned_bam_basename@add_comment
        reference_gz_amb: reference_gz_amb
        reference_gz_sa: reference_gz_sa
        reference_gz_pac: reference_gz_pac
        reference_gz_ann: reference_gz_ann
        reference_gz_bwt: reference_gz_bwt
        reference_gz_fai: reference_gz_fai
        reference_gz: reference_gz

    aligned_bam_qc:
      tool: aligned_bam_qc
      input:
        picard_jar: picard_jar
        aligned_bam: merged_output_bam@pcawg_aligner
        reference_sequence: reference_gz  # may need unzipped version

    aligned_bam_oxog_metrics:
      tool: aligned_bam_oxog_metrics
      input:
        picard_jar: picard_jar
        aligned_bam: merged_output_bam@pcawg_aligner
        reference_sequence: reference_gz  # may need unzipped version

    # may add more QCs if needed

    create_tar:
      tool: create_tar
      input:
        aliquot_id: aliquot_id@validate_metadata
        number_of_lanes: number_of_lanes@validate_metadata
        util_dckr: util_dckr
        output_dir: output_dir@pcawg_aligner
        task_aligned_bam_qc_outdir: output_dir@aligned_bam_qc
        lane_bam_qc_dir: lane_bam_qc_dir@lane_bam_qc
        task_aligned_bam_oxog_metrics_wkdir: output_dir@aligned_bam_oxog_metrics


    generate_song_payload:
      tool: generate_song_payload
      input:
        util_dckr: util_dckr
        metadata_yaml: metadata_yaml
        bam_file: merged_output_bam@pcawg_aligner
        bai_file: merged_output_bai@pcawg_aligner
        tar_file: tar_file@create_tar
        task_aligned_bam_qc_outdir: output_dir@aligned_bam_qc
        lane_bam_qc_dir: lane_bam_qc_dir@lane_bam_qc
        task_aligned_bam_oxog_metrics_wkdir: output_dir@aligned_bam_oxog_metrics


    upload_song_payload_collab:
      tool: upload_song_payload
      input:
        is_allowed: collab_upload_allowed@validate_metadata
        payload: payload@generate_song_payload
        song_metadata_url: song_collab_url
        study: study@validate_metadata

    #upload_song_payload_aws:
    #  tool: upload_song_payload
    #  input:
    #    is_allowed: aws_upload_allowed@validate_metadata
    #    payload: payload@generate_song_payload
    #    song_metadata_url: song_aws_url
    #    study: study@validate_metadata

    save_song_payload_collab:
      tool: save_song_payload
      input:
        is_allowed: collab_upload_allowed@validate_metadata
        song_metadata_url: song_collab_url
        study: study@validate_metadata
        upload_id: uploadId@upload_song_payload_collab

    #save_song_payload_aws:
    #  tool: save_song_payload
    #  input:
    #    is_allowed: aws_upload_allowed@validate_metadata
    #    song_metadata_url: song_aws_url
    #    study: study@validate_metadata
    #    upload_id: uploadId@upload_song_payload_aws

    create_manifest_file_collab:
      tool: create_manifest_file
      input:
        is_allowed: collab_upload_allowed@validate_metadata
        input_dir: output_dir@pcawg_aligner
        song_metadata_url: song_collab_url
        study: study@validate_metadata
        analysis_id: analysisId@save_song_payload_collab

    #create_manifest_file_aws:
    #  tool: create_manifest_file
    #  input:
    #    is_allowed: aws_upload_allowed@validate_metadata
    #    input_dir: output_dir@pcawg_aligner
    #    song_metadata_url: song_aws_url
    #    study: study@validate_metadata
    #    analysis_id: analysisId@save_song_payload_aws

    score_upload_collab:
      tool: score_upload
      input:
        is_allowed: collab_upload_allowed@validate_metadata
        manifest_file: manifest_file@create_manifest_file_collab
        input_dir: output_dir@pcawg_aligner
        song_metadata_url: song_collab_url

    #score_upload_aws:
    #  tool: score_upload
    #  input:
    #    is_allowed: aws_upload_allowed@validate_metadata
    #    manifest_file: manifest_file@create_manifest_file_aws
    #    input_dir: output_dir@pcawg_aligner
    #    song_metadata_url: song_aws_url

    publish_song_payload_collab:
      tool: publish_song_payload
      input:
        is_allowed: collab_upload_allowed@validate_metadata
        song_metadata_url: song_collab_url
        study: study@validate_metadata
        analysis_id: analysisId@save_song_payload_collab
      depends_on:
      - completed@score_upload_collab

    #publish_song_payload_aws:
    #  tool: publish_song_payload
    #  input:
    #    is_allowed: aws_upload_allowed@validate_metadata
    #    song_metadata_url: song_aws_url
    #    study: study@validate_metadata
    #    analysis_id: analysisId@save_song_payload_aws
    #  depends_on:
    #  - completed@score_upload_aws

    create_cgc_manifest:
        tool: create_cgc_manifest
        input:
          util_dckr: util_dckr
          bam_filename: merged_output_bam@pcawg_aligner
          bai_filename: merged_output_bai@pcawg_aligner
          song_payload: payload@generate_song_payload
        depends_on:
        - completed@publish_song_payload_collab

    cgc_upload:
      tool: cgc_upload
      input:
        util_dckr: util_dckr
        manifest_file: manifest@create_cgc_manifest
        project_name: cgc_project_name
        study: study@validate_metadata
      depends_on:
        - completed@publish_song_payload_collab

# A workflow is made up with one or more tools
# Each tool can have its own docker imagine if desirable
tools:
  validate_metadata:
    command: validate_metadata.py

    input:
      metadata_yaml:
        type: string
        is_file: true

    output:
      input_format:
        type: string
      metadata_json:
        type: string
        is_file: true
      aliquot_id:
        type: string
      number_of_lanes:
        type: integer
      study:
        type: string
      collab_upload_allowed:
        type: string
      aws_upload_allowed:
        type: string
      cgc_upload_allowed:
        type: string

  download:
    command: download.py

    input:
      metadata_json:
        type: string
        is_file: true
      input_format:
        type: string

    output:
      output_dir:
        type: string
      download_files:
        type: array
        items:
          type: object

  fastq_to_sam:
    command: fastq_to_sam.py

    input:
      metadata_json:
        type: string
        is_file: true
      picard_jar:
        type: string
        is_file: true
      download_files:
        type: array
        items:
          type: object
      input_format:
        type: string

    output:
      bams:
        type: array
        items:
          type: string
          is_file: true
          glob_pattern: "*.bam"

  revert_bam:
    command: revert_bam.py

    input:
      metadata_json:
        type: string
        is_file: true
      picard_jar:
        type: string
        is_file: true
      download_files:
        type: array
        items:
          type: object
      input_format:
        type: string

    output:
      unaligned_by_rg_dir:
        type: string

  replace_readgroup:
    command: replace_readgroup.py

    input:
      metadata_json:
        type: string
        is_file: true
      picard_jar:
        type: string
        is_file: true
      unaligned_by_rg_dir:
        type: string
      input_format:
        type: string

    output:
      unaligned_rg_replace_dir:
        type: string

  add_comment:
    command: add_comment.py

    input:
      metadata_json:
        type: string
        is_file: true
      picard_jar:
        type: string
        is_file: true
      unaligned_rg_replace_dir:
        type: string
      input_format:
        type: string
      bams:
        type: array
        items:
          type: string
          is_file: true
          glob_pattern: "*.bam"

    output:
      aligned_bam_basename:
        type: string
      bams:
        type: array
        items:
          type: string
          is_file: true
          glob_pattern: "*.bam"

  lane_bam_qc:
    command: |
      python -c '
      import os
      import sys
      import json
      import subprocess
      lane_bams = sys.argv[1].split(",")

      metrics = []
      bam_dir = None
      for bam in lane_bams:
        metrics_file = "%s.quality_yield_metrics.txt" % bam
        lane_bam_qc_dir = os.path.dirname(os.path.abspath(bam))
        command = "java -jar ${picard_jar} CollectQualityYieldMetrics I=%s O=%s" % (bam, metrics_file)
        try:
          p = subprocess.Popen([command], stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
          stdout, stderr = p.communicate()
        except Exception as e:
          exit(stderr)
        metrics.append(metrics_file)

      print(json.dumps({
        "metrics": metrics,
        "lane_bam_qc_dir": lane_bam_qc_dir
      }))

      ' ${sep=',' lane_bams} > output.json

    input:
      picard_jar:
        type: string
        is_file: true
      lane_bams:
        type: array
        items:
          type: string
          is_file: true
    output:
      metrics:
        type: array
        items:
          type: string
          is_file: true
          glob_pattern: "*.quality_yield_metrics.txt"

  pcawg_aligner:  # this is really a PCAWG BWA-MEM CWL workflow wrapper
    command: |
      pcawg_aligner.py --cwl_file ${cwl_file} \
        --reads ${sep=' ' lane_bams} \
        --output_dir datastore \
        --output_file_basename ${output_file_basename} \
        --reference_gz_amb ${reference_gz_amb} \
        --reference_gz_sa ${reference_gz_sa} \
        --reference_gz_pac ${reference_gz_pac} \
        --reference_gz_ann ${reference_gz_ann} \
        --reference_gz_bwt ${reference_gz_bwt} \
        --reference_gz_fai ${reference_gz_fai} \
        --reference_gz ${reference_gz}
    input:
      cwl_file:
        type: string
        is_file: true
      lane_bams:
        type: array
        items:
          type: string
          is_file: true
      output_file_basename:
        type: string
      reference_gz_amb:
        type: string
        is_file: true
      reference_gz_sa:
        type: string
        is_file: true
      reference_gz_pac:
        type: string
        is_file: true
      reference_gz_ann:
        type: string
        is_file: true
      reference_gz_bwt:
        type: string
        is_file: true
      reference_gz_fai:
        type: string
        is_file: true
      reference_gz:
        type: string
        is_file: true
    output:  # output section is ignored for now
      output_dir:
        type: string
      merged_output_bam:
        type: string
        is_file: true
        value: "${output_file_basename}.bam"
      merged_output_bai:
        type: string
        is_file: true
        value: "${output_file_basename}.bam.bai"
      merged_output_metrics:
        type: string
        is_file: true
        value: "${output_file_basename}.bam.metrics"
      merged_output_stats:
        type: string
        is_file: true
        value: "${output_file_basename}.bam.stats.txt"

  aligned_bam_qc:  # this requires RScript, install it by 'apt install r-base-core'
    command: |
      java -Xms5000m -jar ${picard_jar} CollectMultipleMetrics \
        I=${aligned_bam} \
        O=multiple_metrics \
        R=${reference_sequence} \
        ASSUME_SORTED=true \
        PROGRAM="null" \
        PROGRAM=CollectBaseDistributionByCycle \
        PROGRAM=CollectAlignmentSummaryMetrics \
        PROGRAM=CollectInsertSizeMetrics \
        PROGRAM=MeanQualityByCycle \
        PROGRAM=QualityScoreDistribution \
        PROGRAM=CollectSequencingArtifactMetrics \
        PROGRAM=CollectQualityYieldMetrics \
        METRIC_ACCUMULATION_LEVEL="null" \
        METRIC_ACCUMULATION_LEVEL="ALL_READS" \
        METRIC_ACCUMULATION_LEVEL="SAMPLE" \
        METRIC_ACCUMULATION_LEVEL="LIBRARY" \
        METRIC_ACCUMULATION_LEVEL="READ_GROUP" \
      && echo "{ \"output_dir\": \"$(pwd)\" }" > output.json
    input:
      picard_jar:
        type: string
        is_file: true
      aligned_bam:
        type: string
      reference_sequence:
        type: string
        is_file: true
    output:
      output_dir:
        type: string
      metrics_files:
        type: array
        items:
          type: string
          is_file: true
          glob_pattern: "multiple_metrics*.txt"

  aligned_bam_oxog_metrics:
    command: |
      java -jar ${picard_jar} CollectOxoGMetrics \
        I=${aligned_bam} \
        O=oxoG_metrics.txt \
        R=${reference_sequence} \
      && echo "{ \"output_dir\": \"$(pwd)\" }" > output.json
    input:
      picard_jar:
        type: string
        is_file: true
      aligned_bam:
        type: string
      reference_sequence:
        type: string
        is_file: true
    output:
      output_dir:
        type: string

  create_tar:
    command: |
      docker pull ${util_dckr} \
      && TAR_NAME=${aliquot_id}.${number_of_lanes}.$(date +%Y%m%d).wgs.qc_metrics.tgz \
      && docker run \
          --rm \
          --user 1000:1000 \
          --workdir / \
          -v ${output_dir}:/data \
          -v ${task_aligned_bam_qc_outdir}:/aligned_bam_qc \
          -v ${lane_bam_qc_dir}:/unaligned_seq_qc \
          -v ${task_aligned_bam_oxog_metrics_wkdir}:/oxog_metrics \
          ${util_dckr} sh -c "tar czf /data/$TAR_NAME aligned_bam_qc/multiple_metrics.* unaligned_seq_qc/*.lane.bam.quality_yield_metrics.txt oxog_metrics/oxoG_metrics.txt" \
      && echo "{ \"tar_file\": \"${output_dir}/$TAR_NAME\" }" > output.json

    input:
      aliquot_id:
        type: string
      number_of_lanes:
        type: integer
      util_dckr:
        type: string
      output_dir:
        type: string
      task_aligned_bam_qc_outdir:
        type: string
      lane_bam_qc_dir:
        type: string
      task_aligned_bam_oxog_metrics_wkdir:
        type: string

    output:
      tar_file:
        type: string
        is_file: true

  generate_song_payload:
    command: |
      docker pull ${util_dckr} \
      && docker run \
          --rm \
          --user 1000:1000 \
          -v $(pwd):/data \
          -v ${metadata_yaml}:/app/metadata.yaml:ro \
          -v ${bam_file}:/app/$(basename ${bam_file}):ro \
          -v ${bai_file}:/app/$(basename ${bai_file}):ro \
          -v ${tar_file}:/app/$(basename ${tar_file}):ro \
          -v ${lane_bam_qc_dir}:/lane_unaligned \
          -v ${task_aligned_bam_oxog_metrics_wkdir}:/task_aligned_bam_oxog_metrics_wkdir \
          -v ${task_aligned_bam_qc_outdir}:/task_aligned_bam_qc_outdir \
          ${util_dckr} python3 generate_song_payload.py \
            metadata.yaml \
            $(basename ${bam_file}) \
            $(basename ${bai_file}) \
            $(basename ${tar_file}) \
            /lane_unaligned \
            /task_aligned_bam_oxog_metrics_wkdir \
            /task_aligned_bam_qc_outdir \
            --wf-name ${_wf_name} \
            --wf-version ${_wf_version} \
            --wf-execution-runner-version ${_jt_exec_version} \
            --wf-execution-job_id ${_job_id} > payload.json \
      && echo "{ \"payload\": \"$(pwd)/payload.json\" }" > output.json

    input:
      util_dckr:
        type: string
      metadata_yaml:
        type: string
        is_file: true
      bam_file:
        type: string
        is_file: true
      bai_file:
        type: string
        is_file: true
      tar_file:
        type: string
        is_file: true
      lane_bam_qc_dir:
        type: string
      task_aligned_bam_oxog_metrics_wkdir:
        type: string
      task_aligned_bam_qc_outdir:
        type: string
    output:
      payload:
        type: string
        is_file: true

  upload_song_payload:
    command: |
      python -c '
      import os
      import sys
      import requests
      import json

      if ("${is_allowed}" != "True"):
        with open("output.json", "w") as o:
          o.write(json.dumps({"_skipped": True, "status": "", "uploadId": ""}))
        sys.exit(0)

      headers = {
        "Authorization": "Bearer %s" % os.environ["ACCESSTOKEN"],
        "Content-Type": "application/json",
        "Accept": "application/json"
      }

      with open("${payload}", "r") as f:
        payload_str = f.read()

      res = requests.post("%s/upload/%s" % ("${song_metadata_url}", "${study}"),
              data=payload_str,
              headers=headers)

      if res.status_code == 200:
        with open("output.json", "w") as o:
          o.write(json.dumps(res.json()))
      else:
        sys.exit("SONG upload failed: %s" % res)
      '

    input:
      is_allowed:
        type: tring
      payload:
        type: string
        is_file: true
      song_metadata_url:
        type: string
      study:
        type: string
    output:
      status:
        type: string
      uploadId:
        type: string

  save_song_payload:
    command: |
      python -c '
      import os
      import sys
      import requests
      import json

      if ("${is_allowed}" != "True"):
        with open("output.json", "w") as o:
          o.write(json.dumps({"_skipped": True, "status": "", "analysisId": ""}))
        sys.exit(0)

      headers = {
        "Authorization": "Bearer %s" % os.environ["ACCESSTOKEN"],
        "Content-Type": "application/json",
        "Accept": "application/json"
      }

      res = requests.post("%s/upload/%s/save/%s" % ("${song_metadata_url}", "${study}", "${upload_id}"),
              headers=headers)

      if res.status_code >= 200 and res.status_code < 300:
        with open("output.json", "w") as o:
          o.write(json.dumps(res.json()))
      else:
        sys.exit("SONG save failed: %s" % res)
      '

    input:
      is_allowed:
        type: tring
      song_metadata_url:
        type: string
      study:
        type: string
      upload_id:
        type: string
    output:
      status:
        type: string
      analysisId:
        type: string

  create_manifest_file:
    command: |
      python -c '
      import os
      import sys
      import requests
      import json

      if ("${is_allowed}" != "True"):
        with open("output.json", "w") as o:
          o.write(json.dumps({"_skipped": True, "manifest_file": ""}))
        sys.exit(0)

      res = requests.get("%s/studies/%s/analysis/%s" % ("${song_metadata_url}", "${study}", "${analysis_id}"))

      if res.status_code == 200:
        song_metadata = res.json()
        manifest_file = os.path.join(os.getcwd(), "manifest.txt")
        with open(manifest_file, "w") as m:
          m.write("%s\t\t\n" % song_metadata["analysisId"])
          for f in song_metadata["file"]:
            m.write("%s\t%s\t%s\n" % (f["objectId"],
                                      os.path.join(os.getcwd(), "..", "task.pcawg_aligner", f["fileName"]),
                                      f["fileMd5sum"]))

        with open("output.json", "w") as o:
          o.write(json.dumps({"manifest_file": manifest_file}))
      else:
        sys.exit("Saved SONG analysis could not be found (should never happen): %s" % res)
      '

    input:
      is_allowed:
        type: tring
      input_dir:
        type: string
      song_metadata_url:
        type: string
      study:
        type: string
      analysis_id:
        type: string
    output:
      manifest_file:
        type: string
        is_file: true

  score_upload:
    command: |
      bash -c '
      set -euxo pipefail
      if [ ${is_allowed} = True ]; then
        profile="collab"
        if [[ ${song_metadata_url} == *"virginia"* ]]; then
          profile="aws"
        fi

        score-client --profile $profile upload --manifest ${manifest_file}

        echo "{ \"_skipped\": false }" > output.json;

      else
        echo "{ \"_skipped\": true }" > output.json;
      fi
      '

    input:
      is_allowed:
        type: string
      manifest_file:
        type: string
        is_file: true
      song_metadata_url:
        type: string

  create_cgc_manifest:
    command: |
      bash -c '
      set -euxo pipefail
      docker pull ${util_dckr}
      docker run \
        -v $(pwd):/data \
        -v ${song_payload}:/data/song_payload.json \
        ${util_dckr} \
        ./generate_cgc_manifest.py \
        --filenames ${bam_filename} ${bai_filename} \
        --song-payload /data/song_payload.json \
        --output /data/manifest.txt
      echo "{ \"manifest\": \"$(pwd)/manifest.txt\" }" > output.json
      '

    input:
      util_dckr:
        type: string
      bam_filename:
        type: string
      bai_filename:
        type: string
      song_payload:
        type: string
    output:
      manifest:
        type: string

  cgc_upload:
    command: |
      bash -c '
      set -euxo pipefail
      docker pull ${util_dckr}
      docker run -e CGC_ACCESS_TOKEN -v ${manifest_file}:/data/manifest.csv:ro ${util_dckr} \
      cgc-uploader.sh \
      -t $CGC_ACCESS_TOKEN \
      -p ${project_name} \
      -f ${study} \
      -mf /data/manifest.csv \
      -mm experimental_strategy case_id aliquot_uuid \
          case_submitter_id sample_class study ftype
      '

    input:
      util_dckr:
        type: string
      project_name:
        type: string
      manifest_file:
        type: string
      study:
        type: string

  publish_song_payload:
    command: |
      python -c '
      import os
      import sys
      import requests
      import json

      if ("${is_allowed}" != "True"):
        with open("output.json", "w") as o:
          o.write(json.dumps({ "_skipped": True }))
        sys.exit(0)

      headers = {
        "Authorization": "Bearer %s" % os.environ["ACCESSTOKEN"],
        "Content-Type": "application/json",
        "Accept": "application/json"
      }

      res = requests.put("%s/studies/%s/analysis/publish/%s" % \
                          ("${song_metadata_url}", "${study}", "${analysis_id}"), \
                          headers=headers)

      if res.status_code >= 200 and res.status_code < 300:
        print("Publishing succeeded for ${analysis_id}: %s" % res)
        with open("output.json", "w") as o:
          o.write(json.dumps({ "_skipped": False, "status": "ok" }))
      else:
        sys.exit("SONG publishing failed: %s" % res)
      '

    input:
      is_allowed:
        type: tring
      song_metadata_url:
        type: string
      study:
        type: string
      analysis_id:
        type: string
